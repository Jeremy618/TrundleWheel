{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31ea1e09",
   "metadata": {},
   "source": [
    "# Final CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ab0f7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7c0f3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re\n",
    "# import csv\n",
    "# from datetime import datetime\n",
    "# import folium\n",
    "# from folium import plugins\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten  \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\jerem\\\\Documents\\\\TrundleWheel\\\\Data\\\\User1_Jeremy_Bezancon\\\\all_datas_2\" # without GPS datas\n",
    "all_data = pd.read_csv(file_path+'.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "65708d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_start =  [0, 45707, 94349, 141283, 204708, 267536, 314470, 354269, 389073, 425639, 470657, 519844, 572119, 624270, 669829, 716147]\n",
      "test_start =  [31307, 79949, 126883, 190308, 253136, 300070, 339869, 374673, 411239, 456257, 505444, 557719, 609870, 655429, 701747]\n",
      "X_train shape: (347988, 6)\n",
      "y_train shape: (347988,)\n",
      "X_valid shape: (150000, 6)\n",
      "y_valid shape: (150000,)\n",
      "X_test shape: (216000, 6)\n",
      "y_test shape: (216000,)\n"
     ]
    }
   ],
   "source": [
    "# Find indexes of valid, train and test data in the \"all_datas_2\" file\n",
    "\n",
    "walk_start = [0] # indexes where each walk begin \n",
    "test_start = [] # indexes where each test data begin (4 last minutes of each walk (= 1 hour))\n",
    "chunk_test = 14400 # = 60*60*4 = 4 minutes\n",
    "\n",
    "for i in range(len(all_data) - 1):\n",
    "    current_value = all_data.iloc[i, 0]\n",
    "    next_value = all_data.iloc[i + 1, 0]\n",
    "\n",
    "    if current_value > next_value:\n",
    "        walk_start.append(i-1)\n",
    "        test_start.append(i-(chunk_test)-1);\n",
    "        \n",
    "# add the last walk index\n",
    "walk_start.append(len(all_data)-1)\n",
    "test_start.append(len(all_data) - (chunk_test)-1)\n",
    "        \n",
    "print(\"walk_start = \",walk_start)\n",
    "print(\"test_start = \",test_start)\n",
    "\n",
    "\n",
    "# Split the data:\n",
    "#  - Test data = 1 hour (4 mins of each walk)\n",
    "#  - Train data = 70% of remaining data \n",
    "#  - Valid data = 30% of remaining data \n",
    "\n",
    "# Columns 0 to 11 : [time, m_acc_x, m_acc_y, m_acc_z, m_accG_x, m_accG_y, m_accG_z, m_rotTate_alpha, m_rotTate_beta, m_rotTate_gamma, steps, distance]\n",
    "col_s = 4 # first column\n",
    "col_f = 10 # last column\n",
    "\n",
    "X_train_list = []\n",
    "Y_train_list = []\n",
    "X_valid_list = []\n",
    "Y_valid_list = []\n",
    "X_test_list = []\n",
    "Y_test_list = []\n",
    "\n",
    "# number of rows of train and valid data given to the algorithm at eact itteration in the loop\n",
    "chunk = 300 # 300 = 60*5 = 5 secs\n",
    "\n",
    "for walk_idx in range(len(walk_start)-1):\n",
    "    \n",
    "    # for train and valid data\n",
    "    start_idx = walk_start[walk_idx]\n",
    "    end_idx = test_start[walk_idx]\n",
    "    split_point = start_idx + int((end_idx-start_idx) * 0.7) # index where train data stop and valid data start\n",
    "    # Ajust split_point and end_idx because valid and train data must be divisible by chunks\n",
    "    split_point = split_point - (split_point-start_idx)%chunk \n",
    "    end_idx = end_idx - (end_idx-split_point)%chunk\n",
    "    \n",
    "    X_train_list.append(all_data.iloc[start_idx:split_point, col_s:col_f])\n",
    "    Y_train_list.append(all_data.iloc[start_idx:split_point]['distance'])   \n",
    "    X_valid_list.append(all_data.iloc[split_point:end_idx, col_s:col_f])\n",
    "    Y_valid_list.append(all_data.iloc[split_point:end_idx]['distance'])\n",
    "    \n",
    "    \n",
    "    # for test data\n",
    "    start_idx = test_start[walk_idx]\n",
    "    end_idx = walk_start[walk_idx+1]\n",
    "    X_test_list.append(all_data.iloc[start_idx:end_idx, col_s:col_f])\n",
    "    Y_test_list.append(all_data.iloc[start_idx:end_idx]['distance'])\n",
    "    \n",
    "X_train = pd.concat(X_train_list, ignore_index=True)\n",
    "y_train = pd.concat(Y_train_list, ignore_index=True)\n",
    "X_valid = pd.concat(X_valid_list, ignore_index=True)\n",
    "y_valid = pd.concat(Y_valid_list, ignore_index=True)\n",
    "X_test = pd.concat(X_test_list, ignore_index=True)\n",
    "y_test = pd.concat(Y_test_list, ignore_index=True)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "print(\"y_valid shape:\", y_valid.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "6e6dfa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_valid_normalized = scaler.transform(X_valid)\n",
    "X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "335cc3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_32 (Dense)            (None, 300, 32)           224       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 300, 32)           1056      \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 9600)              0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 1)                 9601      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10881 (42.50 KB)\n",
      "Trainable params: 10881 (42.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define CNN parameters:\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32,input_shape = (chunk, col_f-col_s),kernel_initializer = 'uniform',activation='relu'))\n",
    "model.add(Dense(32,kernel_initializer = 'uniform',activation='relu'))\n",
    "# model.add(Dense(1,kernel_initializer = 'uniform'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='linear'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer = Adam(0.01),loss = 'mse',metrics=['mse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8b49ed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  /  347400\n",
      "i =  0 y_t =  4.587245934959329\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 125.0558 - mse: 125.0558 - val_loss: 128.7297 - val_mse: 128.7297\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 124.4222 - mse: 124.4222 - val_loss: 128.0902 - val_mse: 128.0902\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 123.7935 - mse: 123.7935 - val_loss: 127.4559 - val_mse: 127.4559\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 123.1699 - mse: 123.1699 - val_loss: 126.8265 - val_mse: 126.8265\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 122.5512 - mse: 122.5512 - val_loss: 126.2022 - val_mse: 126.2022\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 121.9376 - mse: 121.9376 - val_loss: 125.5829 - val_mse: 125.5829\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 121.3288 - mse: 121.3288 - val_loss: 124.9686 - val_mse: 124.9686\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 120.7250 - mse: 120.7250 - val_loss: 124.3592 - val_mse: 124.3592\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 120.1261 - mse: 120.1261 - val_loss: 123.7548 - val_mse: 123.7548\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 119.5320 - mse: 119.5320 - val_loss: 123.1553 - val_mse: 123.1553\n",
      "300  /  347400\n",
      "i =  300 y_t =  5.433347350714911\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 138.1140 - mse: 138.1140 - val_loss: 157.2028 - val_mse: 157.2028\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 137.4795 - mse: 137.4795 - val_loss: 156.5252 - val_mse: 156.5252\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 136.8459 - mse: 136.8459 - val_loss: 155.8489 - val_mse: 155.8489\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 136.2136 - mse: 136.2136 - val_loss: 155.1743 - val_mse: 155.1743\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 135.5830 - mse: 135.5830 - val_loss: 154.5017 - val_mse: 154.5017\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 134.9544 - mse: 134.9544 - val_loss: 153.8315 - val_mse: 153.8315\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 134.3281 - mse: 134.3281 - val_loss: 153.1642 - val_mse: 153.1642\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 133.7045 - mse: 133.7045 - val_loss: 152.4998 - val_mse: 152.4998\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 133.0838 - mse: 133.0838 - val_loss: 151.8385 - val_mse: 151.8385\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 132.4660 - mse: 132.4660 - val_loss: 151.1806 - val_mse: 151.1806\n",
      "600  /  347400\n",
      "i =  600 y_t =  5.396798029556647\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 131.0136 - mse: 131.0136 - val_loss: 149.3791 - val_mse: 149.3791\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 130.4047 - mse: 130.4047 - val_loss: 148.7312 - val_mse: 148.7312\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 129.7995 - mse: 129.7995 - val_loss: 148.0875 - val_mse: 148.0875\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 129.1982 - mse: 129.1982 - val_loss: 147.4477 - val_mse: 147.4477\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 128.6006 - mse: 128.6006 - val_loss: 146.8121 - val_mse: 146.8121\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 128.0071 - mse: 128.0071 - val_loss: 146.1808 - val_mse: 146.1808\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 127.4176 - mse: 127.4176 - val_loss: 145.5536 - val_mse: 145.5536\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 126.8321 - mse: 126.8321 - val_loss: 144.9308 - val_mse: 144.9308\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 126.2507 - mse: 126.2507 - val_loss: 144.3123 - val_mse: 144.3123\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 125.6735 - mse: 125.6735 - val_loss: 143.6981 - val_mse: 143.6981\n",
      "900  /  347400\n",
      "i =  900 y_t =  5.747023809523849\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 133.0576 - mse: 133.0576 - val_loss: 124.1700 - val_mse: 124.1700\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 132.4690 - mse: 132.4690 - val_loss: 123.6028 - val_mse: 123.6028\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 131.8832 - mse: 131.8832 - val_loss: 123.0385 - val_mse: 123.0385\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 131.3002 - mse: 131.3002 - val_loss: 122.4771 - val_mse: 122.4771\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 130.7202 - mse: 130.7202 - val_loss: 121.9188 - val_mse: 121.9188\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 130.1435 - mse: 130.1435 - val_loss: 121.3639 - val_mse: 121.3639\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 129.5701 - mse: 129.5701 - val_loss: 120.8120 - val_mse: 120.8120\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 128.9999 - mse: 128.9999 - val_loss: 120.2638 - val_mse: 120.2638\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 128.4333 - mse: 128.4333 - val_loss: 119.7189 - val_mse: 119.7189\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 127.8703 - mse: 127.8703 - val_loss: 119.1776 - val_mse: 119.1776\n",
      "1200  /  347400\n",
      "i =  1200 y_t =  6.077110389610308\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 134.8686 - mse: 134.8686 - val_loss: 125.4779 - val_mse: 125.4779\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 134.2950 - mse: 134.2950 - val_loss: 124.9258 - val_mse: 124.9258\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 133.7238 - mse: 133.7238"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[329], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m X_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(X_v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     35\u001b[0m y_v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(y_v, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_t,y_t,epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,validation_data \u001b[38;5;241m=\u001b[39m(X_v,y_v))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:1791\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1777\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1778\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         pss_evaluation_shards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pss_evaluation_shards,\n\u001b[0;32m   1790\u001b[0m     )\n\u001b[1;32m-> 1791\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m   1792\u001b[0m     x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1793\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   1794\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[0;32m   1795\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mvalidation_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[0;32m   1796\u001b[0m     steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m   1797\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1798\u001b[0m     max_queue_size\u001b[38;5;241m=\u001b[39mmax_queue_size,\n\u001b[0;32m   1799\u001b[0m     workers\u001b[38;5;241m=\u001b[39mworkers,\n\u001b[0;32m   1800\u001b[0m     use_multiprocessing\u001b[38;5;241m=\u001b[39muse_multiprocessing,\n\u001b[0;32m   1801\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1802\u001b[0m     _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1803\u001b[0m )\n\u001b[0;32m   1804\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1806\u001b[0m }\n\u001b[0;32m   1807\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:2189\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test_counter\u001b[38;5;241m.\u001b[39massign(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2188\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_test_begin()\n\u001b[1;32m-> 2189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m   2190\u001b[0m     _,\n\u001b[0;32m   2191\u001b[0m     dataset_or_iterator,\n\u001b[0;32m   2192\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39menumerate_epochs():  \u001b[38;5;66;03m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\data_adapter.py:1331\u001b[0m, in \u001b[0;36mDataHandler.enumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_truncate_execution_to_epoch():\n\u001b[1;32m-> 1331\u001b[0m     data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset)\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epochs):\n\u001b[0;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:506\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    509\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:710\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    706\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 710\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_iterator(dataset)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    746\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m    747\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[0;32m    748\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[1;32m--> 749\u001b[0m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmake_iterator(ds_variant, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3451\u001b[0m, in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m   3450\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3451\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[0;32m   3452\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMakeIterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, dataset, iterator)\n\u001b[0;32m   3453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3454\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run the CNN\n",
    "\n",
    "#fix random seed for reproductability\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "hist = []\n",
    "\n",
    "for i in range(0, len(X_train) - chunk, chunk):  \n",
    "# for i in range(0, 20000, chunk):  \n",
    "    \n",
    "    print(i,' / ',len(X_train) - chunk)\n",
    "    \n",
    "    # new train and valid data for each itteration in the loop  \n",
    "    X_t = X_train_normalized[i:i+chunk, :] \n",
    "    y_t = y_train[i+chunk-1] - y_train[i] # distance walked during 5 secs\n",
    "    print(\"i = \",i,\"y_t = \",y_t)\n",
    "    if(y_t>0): # if not: distance difference between two differents walks\n",
    "        X_t = np.array(X_t)\n",
    "        y_t = np.array(y_t)\n",
    "        y_t = y_t.reshape(-1, 1)\n",
    "\n",
    "        # select random valids values among valid data\n",
    "        index_v = int(np.random.rand()*(len(X_valid_normalized)-301))\n",
    "        X_v = X_valid_normalized[index_v:index_v+chunk, :] \n",
    "        y_v = y_valid[index_v+chunk] - y_valid[index_v]\n",
    "        X_v = np.array(X_v)\n",
    "        y_v = np.array(y_v)\n",
    "        y_v = y_v.reshape(-1, 1)\n",
    "\n",
    "        X_t = np.expand_dims(X_t, axis=0)\n",
    "        y_t = np.expand_dims(y_t, axis=0)\n",
    "\n",
    "        X_v = np.expand_dims(X_v, axis=0)\n",
    "        y_v = np.expand_dims(y_v, axis=0)\n",
    "\n",
    "        model.fit(X_t,y_t,epochs = 10,batch_size = 10,validation_data =(X_v,y_v))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "25977dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "493.277777777778"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[45599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272054d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction:\n",
    "\n",
    "\n",
    "# init tables    \n",
    "y_p = np.zeros((15, int((len(X_test) - chunk)/(chunk*15))+1))\n",
    "y_r = np.zeros((15, int((len(X_test) - chunk)/(chunk*15))+1))\n",
    "# y_p = np.zeros((15, int((len(X_test) - chunk)/chunk)))\n",
    "# y_r = np.zeros((15, int((len(X_test) - chunk)/chunk)))\n",
    "walk = 0\n",
    "\n",
    "print(\"Distance walked during last 4 minutes of each walk :\")\n",
    "\n",
    "c = 0\n",
    "for i in range(0, len(X_test) - chunk, chunk):\n",
    "\n",
    "    if  ((i != 0) and (i%(chunk_test) == 0)): # every 4 minutes\n",
    "        print(\"walk number \",walk)\n",
    "#         print(\"real distance \", y_test[i-1] - y_test[i-chunk_test-1])\n",
    "#         print(\"predicted distance \",y_p[walk]) \n",
    "        walk += 1\n",
    "        c = 0\n",
    "   \n",
    "    test = np.expand_dims(X_test_normalized[i:i+chunk,:], axis=0)\n",
    "    y_p[walk][c] = model.predict(test, verbose = False)\n",
    "    y_r[walk][c] = y_test[i+chunk-1] - y_test[i]\n",
    "#     print(\"y_r = \", y_r[walk][c])\n",
    "#     print(\"y_p = \", y_p[walk][c])\n",
    "    c+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab207",
   "metadata": {},
   "outputs": [],
   "source": [
    "walks_nbr, chunks_nbr = y_p.shape\n",
    "indices_x = np.arange(chunks_nbr)\n",
    "\n",
    "for i in range(walks_nbr):\n",
    "\n",
    "    print(i+1)\n",
    "    plt.plot(indices_x*300/3600, y_r[i], 'r-', label='Real data', linewidth=1)\n",
    "    plt.plot(indices_x*300/3600, y_p[i], 'b-', label='Prediction',linewidth=1)\n",
    "    plt.title('Each distance difference predicted and Real Data ')\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.ylabel('Distance (m)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    sum_r = [y_r[0][0]]\n",
    "    sum_p = [y_p[0][0]]\n",
    "    for j in range(1, chunks_nbr):\n",
    "        sum_r.append(y_r[i][j] + sum_r[-1])\n",
    "        sum_p.append(y_p[i][j] + sum_p[-1])\n",
    "        \n",
    "    plt.plot(indices_x*300/3600, sum_r, 'r-', label='Real data', linewidth=1)\n",
    "    plt.plot(indices_x*300/3600, sum_p, 'b-', label='Prediction',linewidth=1)\n",
    "    plt.title('Total distance predicted and Real Data')\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.ylabel('Distance (m)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4aedd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
